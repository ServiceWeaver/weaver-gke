// Copyright 2022 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

option go_package = "github.com/ServiceWeaver/weaver-gke/internal/nanny/assigner";

package assigner;
import "internal/nanny/nanny.proto";

// ReplicaSetState stores information about a Kubernetes ReplicaSet, i.e.,
// a set of pods that host the same set of components.
message ReplicaSetState {
  nanny.ReplicaSet replica_set = 1;

  // Routing assignments for all components that use routing/sharding, keyed
  // by component name.
  map<string, Assignment> routing_assignments = 2;
}

// Assignment is a wrapper class that creates and decodes assignment protos.
message Assignment {
  uint64 version = 1;
  repeated Slice slices = 2;
  map<string, bool> candidate_resources = 3;
  AlgoConstraints constraints = 4;
  Statistics stats = 5;
}

// SliceKey is an abstraction for routing keys used in slices.
message SliceKey {
  uint64 val = 1;
}

// Slice contains the allocation of a routing key range to a set of resources.
//
// The range covers [startInclusive, endExclusive).
message Slice {
  SliceKey start_inclusive = 1;
  SliceKey end_exclusive = 2;
  LoadTracker load_info = 3;
}

// loadTracker tracks load information for a given slice across all the assigned
// resources.
//
// Note that for a replicated slice (len(resources) > 1):
//   - perResourceload contains the total load as reported by the latest resource
//     that has the slice assigned
//   - distribution contains the load distribution along split points for the
//     given replica
//
// Most of the slices will have a single replica; ideally, only hot keys should
// be replicated. Given that our load metric is req/s it is fair to assume that
// the load is evenly distributed across the replicas of a slice, hence the
// load, and the load distribution for a given replica should be representative
// for all replicas. Otherwise, it is challenging for the weavelets to report
// exactly the same split points for all replicas, and challenging for the
// assigner to compute split points that make sense across all replicas.
//
// TODO(rgrandl): mwhittaker@ has an interesting idea. Instead of taking the
// latest load report, we take the load report with the largest number of
// requests received. That way if a resource was down for some reason or lagging
// behind in an assignment and reports an almost empty load report, we won't use
// it.
//
// TODO(rgrandl): revisit these decisions if we ever decide to support a
// different load metric.
message LoadTracker {
  // Slice load on a given resource.
  double per_resource_load = 1;

  // Resources to which the slice is assigned.
  map<string, bool> resources = 2;

  // Distribution of the load along split points.
  map<uint64, double> distribution = 3;
}

// AlgoConstraints contains various constraints needed by the routing algos to
// generate assignments.
message AlgoConstraints {
  // Upper bound on the load allowed on any resource.
  double max_load_limit_resource = 1;

  // Lower bound on the load allowed on any resource.
  double min_load_limit_resource = 2;

  // Any slice with replica load above this threshold should be split to enable
  // the algo to move slices around, in order to provide load balancing
  // guarantees.
  double split_threshold = 3;

  // Any slice with replica load above this threshold should be replicated to
  // enable the algo to move slices around, in order to provide load balancing
  // guarantees.
  //
  // In practice, this threshold should be set to a value equal to or higher
  // than the splitThreshold, because it is preferred for a slice to be split
  // first.
  //
  // Also, a good algorithm should replicate a slice iff the slice has a single
  // key; otherwise splitting is always preferred because it incurs less churn.
  double replicate_threshold = 4;

  // Any slice with replica load below this threshold should be dereplicated if
  // the number of replicas is greater than 1.
  //
  // Dereplication creates more opportunities for merging slices, hence
  // controlling the assignment size.
  double dereplicate_threshold = 5;

  // Maximum number of slices a resource should be assigned in a given
  // assignment. Note that this is just a hint, because in reality the algorithm
  // might not be able to provide a hard guarantee.
  //
  // The role of this constraint is to ensure that the number of slices in an
  // assignment is bounded.
  int64 max_num_slices_resource_hint = 6;
}

// Statistics contains various statistics for a given assignment.
message Statistics {
  int64 splitOps = 1;
  int64 mergeOps = 2;
  int64 replicateOps = 3;
  int64 dereplicateOps = 4;
  int64 moveDueToBalanceOps = 5;
  int64 moveDueToUnhealthyOps = 6;
}
